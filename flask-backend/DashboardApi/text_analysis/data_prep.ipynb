{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Obtaining dependency information for numpy from https://files.pythonhosted.org/packages/16/2e/86f24451c2d530c88daf997cb8d6ac622c1d40d19f5a031ed68a4b73a374/numpy-1.26.4-cp312-cp312-win_amd64.whl.metadata\n",
      "  Using cached numpy-1.26.4-cp312-cp312-win_amd64.whl.metadata (61 kB)\n",
      "Using cached numpy-1.26.4-cp312-cp312-win_amd64.whl (15.5 MB)\n",
      "Installing collected packages: numpy\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to write executable - trying to use .deleteme logic\n",
      "ERROR: Could not install packages due to an OSError: [WinError 2] The system cannot find the file specified: 'c:\\\\Python312\\\\Scripts\\\\f2py.exe' -> 'c:\\\\Python312\\\\Scripts\\\\f2py.exe.deleteme'\n",
      "\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Obtaining dependency information for pandas from https://files.pythonhosted.org/packages/22/a5/a0b255295406ed54269814bc93723cfd1a0da63fb9aaf99e1364f07923e5/pandas-2.2.2-cp312-cp312-win_amd64.whl.metadata\n",
      "  Downloading pandas-2.2.2-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\python312\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Obtaining dependency information for pytz>=2020.1 from https://files.pythonhosted.org/packages/9c/3d/a121f284241f08268b21359bd425f7d4825cffc5ac5cd0e1b3d82ffd2b10/pytz-2024.1-py2.py3-none-any.whl.metadata\n",
      "  Downloading pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Obtaining dependency information for tzdata>=2022.7 from https://files.pythonhosted.org/packages/65/58/f9c9e6be752e9fcb8b6a0ee9fb87e6e7a1f6bcab2cdc73f02bb7ba91ada0/tzdata-2024.1-py2.py3-none-any.whl.metadata\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pandas-2.2.2-cp312-cp312-win_amd64.whl (11.5 MB)\n",
      "   ---------------------------------------- 0.0/11.5 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.6/11.5 MB 12.9 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 2.0/11.5 MB 21.7 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 4.2/11.5 MB 30.0 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 7.0/11.5 MB 37.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.1/11.5 MB 46.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.5/11.5 MB 50.4 MB/s eta 0:00:00\n",
      "Downloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "   ---------------------------------------- 0.0/505.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 505.5/505.5 kB ? eta 0:00:00\n",
      "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.2.2 pytz-2024.1 tzdata-2024.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard library\n",
    "from typing import List\n",
    "\n",
    "# data wrangling\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:6: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:6: SyntaxWarning: invalid escape sequence '\\D'\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_16456\\486083168.py:6: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  csv_path = 'D:/AmritaUniversity/AmmachiLabs/CapacityBuildingPortal/flask-backend\\DashboardApi/text_analysis/osdg_community_dataset.csv'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (32120, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doi</th>\n",
       "      <th>text_id</th>\n",
       "      <th>text</th>\n",
       "      <th>sdg</th>\n",
       "      <th>labels_negative</th>\n",
       "      <th>labels_positive</th>\n",
       "      <th>agreement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.6027/9789289342698-7-en</td>\n",
       "      <td>00021941702cd84171ff33962197ca1f</td>\n",
       "      <td>From a gender perspective, Paulgaard points ou...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.18356/eca72908-en</td>\n",
       "      <td>00028349a7f9b2485ff344ae44ccfd6b</td>\n",
       "      <td>Labour legislation regulates maximum working h...</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.1787/9789264289062-4-en</td>\n",
       "      <td>0004eb64f96e1620cd852603d9cbe4d4</td>\n",
       "      <td>The average figure also masks large difference...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.1787/5k9b7bn5qzvd-en</td>\n",
       "      <td>0006a887475ccfa5a7f5f51d4ac83d02</td>\n",
       "      <td>The extent to which they are akin to corruptio...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.1787/9789264258211-6-en</td>\n",
       "      <td>0006d6e7593776abbdf4a6f985ea6d95</td>\n",
       "      <td>A region reporting a higher rate will not earn...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          doi                           text_id  \\\n",
       "0  10.6027/9789289342698-7-en  00021941702cd84171ff33962197ca1f   \n",
       "1        10.18356/eca72908-en  00028349a7f9b2485ff344ae44ccfd6b   \n",
       "2  10.1787/9789264289062-4-en  0004eb64f96e1620cd852603d9cbe4d4   \n",
       "3     10.1787/5k9b7bn5qzvd-en  0006a887475ccfa5a7f5f51d4ac83d02   \n",
       "4  10.1787/9789264258211-6-en  0006d6e7593776abbdf4a6f985ea6d95   \n",
       "\n",
       "                                                text  sdg  labels_negative  \\\n",
       "0  From a gender perspective, Paulgaard points ou...    5                1   \n",
       "1  Labour legislation regulates maximum working h...   11                2   \n",
       "2  The average figure also masks large difference...    3                1   \n",
       "3  The extent to which they are akin to corruptio...    3                1   \n",
       "4  A region reporting a higher rate will not earn...    3                2   \n",
       "\n",
       "   labels_positive  agreement  \n",
       "0                7   0.750000  \n",
       "1                1   0.333333  \n",
       "2                6   0.714286  \n",
       "3                2   0.333333  \n",
       "4                2   0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_osdg = pd.read_csv('https://zenodo.org/record/5550238/files/osdg-community-dataset-v21-09-30.csv?download=1', sep='\\t')\n",
    "print('Shape:', df_osdg.shape)\n",
    "display(df_osdg.head())\n",
    "\n",
    "# Save to CSV and Excel\n",
    "csv_path = 'D:/AmritaUniversity/AmmachiLabs/CapacityBuildingPortal/flask-backend\\DashboardApi/text_analysis/osdg_community_dataset.csv'\n",
    "# excel_path = 'path_to_save/osdg_community_dataset.xlsx'\n",
    "\n",
    "# D:\\AmritaUniversity\\AmmachiLabs\\CapacityBuildingPortal\\flask-backend\\DashboardApi\\text_analysis\\data_prep.ipynb\n",
    "\n",
    "df_osdg.to_csv(csv_path, index=False)\n",
    "# df_osdg.to_excel(excel_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:5: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:42: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:42: SyntaxWarning: invalid escape sequence '\\D'\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_16456\\212476321.py:5: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  df_osdg = pd.read_csv('D:/AmritaUniversity/AmmachiLabs/CapacityBuildingPortal/flask-backend\\DashboardApi/text_analysis/osdg_community_dataset.csv')\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_16456\\212476321.py:42: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  combined_csv_path_large = 'D:/AmritaUniversity/AmmachiLabs/CapacityBuildingPortal/flask-backend\\DashboardApi/text_analysis/osdg_community_dataset_updated.csv'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset saved to: D:/AmritaUniversity/AmmachiLabs/CapacityBuildingPortal/flask-backend\\DashboardApi/text_analysis/osdg_community_dataset_updated.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "df_osdg = pd.read_csv('D:/AmritaUniversity/AmmachiLabs/CapacityBuildingPortal/flask-backend\\DashboardApi/text_analysis/osdg_community_dataset.csv')\n",
    "\n",
    "# Define the number of samples for SDG 16 and SDG 17\n",
    "num_samples = 200\n",
    "\n",
    "# Function to create synthetic data\n",
    "def create_synthetic_data(sdg, num_samples=200):\n",
    "    return {\n",
    "        'doi': ['synthetic'] * num_samples,\n",
    "        'text_id': [f'synthetic_{sdg}_{i}' for i in range(num_samples)],\n",
    "        'text': [\n",
    "            f\"Promote peaceful and inclusive societies for sustainable development. {i}\" if sdg == 16 else\n",
    "            f\"Strengthen the means of implementation and revitalize the global partnership for sustainable development. {i}\" \n",
    "            for i in range(num_samples)\n",
    "        ],\n",
    "        'sdg': [sdg] * num_samples,\n",
    "        'labels_negative': [np.random.choice(['corruption', 'violence', 'inequality']) for _ in range(num_samples)],\n",
    "        'labels_positive': [\n",
    "            \"peace, justice, strong institutions\" if sdg == 16 else\n",
    "            \"partnership, sustainable development, cooperation\"\n",
    "            for _ in range(num_samples)\n",
    "        ],\n",
    "        'agreement': [np.random.uniform(0.7, 1.0) for _ in range(num_samples)]\n",
    "    }\n",
    "\n",
    "# Create synthetic data for SDG 16 and SDG 17\n",
    "data_sdg_16 = create_synthetic_data(16)\n",
    "data_sdg_17 = create_synthetic_data(17)\n",
    "\n",
    "# Convert to DataFrames\n",
    "df_sdg_16 = pd.DataFrame(data_sdg_16)\n",
    "df_sdg_17 = pd.DataFrame(data_sdg_17)\n",
    "\n",
    "# Append the new data to the existing dataset\n",
    "df_combined = pd.concat([df_osdg, df_sdg_16, df_sdg_17], ignore_index=True)\n",
    "\n",
    "# Save the combined dataset to a new CSV file\n",
    "combined_csv_path_large = 'D:/AmritaUniversity/AmmachiLabs/CapacityBuildingPortal/flask-backend\\DashboardApi/text_analysis/osdg_community_dataset_updated.csv'\n",
    "df_combined.to_csv(combined_csv_path_large, index=False)\n",
    "\n",
    "print(f\"Combined dataset saved to: {combined_csv_path_large}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:5: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:59: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:59: SyntaxWarning: invalid escape sequence '\\D'\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_25876\\1942186870.py:5: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  df_osdg = pd.read_csv('D:/AmritaUniversity/AmmachiLabs/CapacityBuildingPortal/flask-backend\\DashboardApi/text_analysis/osdg_community_dataset.csv')\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_25876\\1942186870.py:59: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  combined_csv_path_large = 'D:/AmritaUniversity/AmmachiLabs/CapacityBuildingPortal/flask-backend\\DashboardApi/text_analysis/osdg_community_dataset_final.csv'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset saved to: D:/AmritaUniversity/AmmachiLabs/CapacityBuildingPortal/flask-backend\\DashboardApi/text_analysis/osdg_community_dataset_final.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "df_osdg = pd.read_csv('D:/AmritaUniversity/AmmachiLabs/CapacityBuildingPortal/flask-backend\\DashboardApi/text_analysis/osdg_community_dataset.csv')\n",
    "\n",
    "# Define the number of samples for SDG 16 and SDG 17\n",
    "num_samples = 200\n",
    "\n",
    "# Lists of example phrases for SDG 16 and SDG 17\n",
    "sdg_16_phrases = [\n",
    "    \"Promote peaceful and inclusive societies for sustainable development.\",\n",
    "    \"Provide access to justice for all.\",\n",
    "    \"Build effective, accountable, and inclusive institutions.\",\n",
    "    \"Reduce violence and combat crime.\",\n",
    "    \"Ensure equal access to justice.\",\n",
    "    \"Strengthen the rule of law and promote human rights.\"\n",
    "]\n",
    "\n",
    "sdg_17_phrases = [\n",
    "    \"Strengthen the means of implementation.\",\n",
    "    \"Revitalize the global partnership for sustainable development.\",\n",
    "    \"Encourage public-private partnerships.\",\n",
    "    \"Enhance international support for sustainable development.\",\n",
    "    \"Promote effective cooperation between countries.\",\n",
    "    \"Support the building of inclusive global partnerships.\"\n",
    "]\n",
    "\n",
    "# Function to create synthetic data\n",
    "def create_synthetic_data(sdg, phrases, num_samples=200):\n",
    "    labels_positive = np.random.randint(5, 20, size=num_samples)  # Random number of positive labels\n",
    "    labels_negative = np.random.randint(0, 5, size=num_samples)   # Random number of negative labels\n",
    "    \n",
    "    # Calculate agreement score\n",
    "    agreement = (labels_positive - labels_negative) / (labels_positive + labels_negative)\n",
    "    \n",
    "    return {\n",
    "        'doi': ['synthetic'] * num_samples,\n",
    "        'text_id': [f'synthetic_{sdg}_{i}' for i in range(num_samples)],\n",
    "        'text': [np.random.choice(phrases) for _ in range(num_samples)],\n",
    "        'sdg': [sdg] * num_samples,\n",
    "        'labels_negative': labels_negative,\n",
    "        'labels_positive': labels_positive,\n",
    "        'agreement': agreement\n",
    "    }\n",
    "\n",
    "# Create synthetic data for SDG 16 and SDG 17\n",
    "data_sdg_16 = create_synthetic_data(16, sdg_16_phrases)\n",
    "data_sdg_17 = create_synthetic_data(17, sdg_17_phrases)\n",
    "\n",
    "# Convert to DataFrames\n",
    "df_sdg_16 = pd.DataFrame(data_sdg_16)\n",
    "df_sdg_17 = pd.DataFrame(data_sdg_17)\n",
    "\n",
    "# Append the new data to the existing dataset\n",
    "df_combined = pd.concat([df_osdg, df_sdg_16, df_sdg_17], ignore_index=True)\n",
    "\n",
    "# Save the combined dataset to a new CSV file\n",
    "combined_csv_path_large = 'D:/AmritaUniversity/AmmachiLabs/CapacityBuildingPortal/flask-backend\\DashboardApi/text_analysis/osdg_community_dataset_final.csv'\n",
    "df_combined.to_csv(combined_csv_path_large, index=False)\n",
    "\n",
    "print(f\"Combined dataset saved to: {combined_csv_path_large}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# Load the combined dataset\n",
    "df_combined = pd.read_csv('D:/AmritaUniversity/AmmachiLabs/CapacityBuildingPortal/flask-backend/DashboardApi/text_analysis/osdg_community_dataset_final.csv')\n",
    "\n",
    "# Convert DataFrame to Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(df_combined)\n",
    "\n",
    "# Load pre-trained tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_test_split = tokenized_datasets.train_test_split(test_size=0.1)\n",
    "train_dataset = train_test_split['train']\n",
    "eval_dataset = train_test_split['test']\n",
    "\n",
    "# Remove the columns not needed for training\n",
    "train_dataset = train_dataset.remove_columns(['doi', 'text_id', 'text', 'labels_negative', 'labels_positive', 'agreement'])\n",
    "eval_dataset = eval_dataset.remove_columns(['doi', 'text_id', 'text', 'labels_negative', 'labels_positive', 'agreement'])\n",
    "\n",
    "# Set the format of the datasets to PyTorch tensors\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'sdg'])\n",
    "eval_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'sdg'])\n",
    "\n",
    "# Load pre-trained model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=17)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained('D:/AmritaUniversity/AmmachiLabs/CapacityBuildingPortal/flask-backend/DashboardApi/text_analysis/')\n",
    "tokenizer.save_pretrained('D:/AmritaUniversity/AmmachiLabs/CapacityBuildingPortal/flask-backend/DashboardApi/text_analysis/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
